{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/4SELECTION-gif/OpenAPI-Specification/blob/main/Financial_Transaction_ETL_Script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Production-quality ETL script for ingesting, cleaning, and normalizing\n",
        "multilingual (Japanese/English) financial transaction CSV files.\n",
        "\n",
        "This script is designed to handle a wide variety of format inconsistencies,\n",
        "including multiple character encodings, extraneous header rows, messy dates, and\n",
        "amount formats, and Unicode normalization issues.\n",
        "\n",
        "It produces a clean, analysis-ready DataFrame and a corresponding CSV file,\n",
        "adhering to a defined target schema and providing detailed quality checks.\n",
        "\"\"\"\n",
        "\n",
        "# 1) Safe Imports & Runtime Setup\n",
        "# ==============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import io\n",
        "import warnings\n",
        "import hashlib\n",
        "import unicodedata\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import List, Optional, Tuple, Dict, Any\n",
        "\n",
        "# Attempt to import zoneinfo, fallback to pytz for older Python versions\n",
        "try:\n",
        "    from zoneinfo import ZoneInfo\n",
        "except ImportError:\n",
        "    from pytz import timezone as ZoneInfo\n",
        "\n",
        "# Pandas display options for readable console summaries\n",
        "pd.set_option('display.max_columns', 50)\n",
        "pd.set_option('display.width', 120)\n",
        "pd.set_option('display.float_format', '{:,.2f}'.format)\n",
        "\n",
        "\n",
        "def zen_to_han(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Converts full-width (zenkaku) characters to half-width (hankaku).\n",
        "    This function uses unicodedata.normalize('NFKC') which is a robust way\n",
        "    to handle Japanese full-width digits, spaces, and ASCII symbols.\n",
        "\n",
        "    Args:\n",
        "        text: The string containing full-width characters.\n",
        "\n",
        "    Returns:\n",
        "        The normalized string with half-width characters.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return text\n",
        "    return unicodedata.normalize('NFKC', text)\n",
        "\n",
        "\n",
        "# 2) File Reading with Encoding Fallback & Header Detection\n",
        "# ==============================================================================\n",
        "def find_best_header(df_peek: pd.DataFrame, max_rows_to_scan: int = 10) -> int:\n",
        "    \"\"\"\n",
        "    Scans the first few rows of a DataFrame to find the most likely header row.\n",
        "    The best header is determined by the row with the highest number of unique,\n",
        "    non-empty values.\n",
        "\n",
        "    Args:\n",
        "        df_peek: A DataFrame read with no header to inspect the first few rows.\n",
        "        max_rows_to_scan: The number of rows to scan from the top.\n",
        "\n",
        "    Returns:\n",
        "        The index of the row most likely to be the header.\n",
        "    \"\"\"\n",
        "    best_header_index = 0\n",
        "    max_score = -1\n",
        "\n",
        "    for i in range(min(max_rows_to_scan, len(df_peek))):\n",
        "        row_values = df_peek.iloc[i].dropna().astype(str)\n",
        "        # Score favors rows with many unique, non-empty values\n",
        "        score = row_values.nunique() + row_values.str.len().sum()\n",
        "        if score > max_score:\n",
        "            max_score = score\n",
        "            best_header_index = i\n",
        "\n",
        "    print(f\"INFO: Best header row detected at index {best_header_index}.\")\n",
        "    return best_header_index\n",
        "\n",
        "\n",
        "def read_transaction_csv(file_path: Path) -> Tuple[pd.DataFrame, str]:\n",
        "    \"\"\"\n",
        "    Reads a CSV file with robust encoding fallback and header detection.\n",
        "\n",
        "    Args:\n",
        "        file_path: The path to the CSV file.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the loaded DataFrame and the detected encoding.\n",
        "    \"\"\"\n",
        "    encodings_to_try = [\"utf-8-sig\", \"utf-8\", \"cp932\", \"shift_jis\"]\n",
        "    detected_encoding = None\n",
        "    df = None\n",
        "\n",
        "    for encoding in encodings_to_try:\n",
        "        try:\n",
        "            # Initial read to detect header\n",
        "            df_peek = pd.read_csv(\n",
        "                file_path,\n",
        "                header=None,\n",
        "                nrows=10,\n",
        "                encoding=encoding,\n",
        "                dtype=str,\n",
        "                keep_default_na=False,\n",
        "                skip_blank_lines=False\n",
        "            )\n",
        "            header_row = find_best_header(df_peek)\n",
        "\n",
        "            # Full read with detected header\n",
        "            df = pd.read_csv(\n",
        "                file_path,\n",
        "                header=header_row,\n",
        "                encoding=encoding,\n",
        "                dtype=str,\n",
        "                keep_default_na=False,\n",
        "                skip_blank_lines=True\n",
        "            )\n",
        "            detected_encoding = encoding\n",
        "            print(f\"INFO: Successfully read file with encoding '{encoding}'.\")\n",
        "            break\n",
        "        except (UnicodeDecodeError, pd.errors.ParserError) as e:\n",
        "            warnings.warn(f\"WARNING: Failed to read with encoding '{encoding}': {e}\")\n",
        "        except FileNotFoundError:\n",
        "            raise\n",
        "\n",
        "    if df is None:\n",
        "        raise ValueError(f\"CRITICAL: Could not read the file at {file_path} with any of the attempted encodings.\")\n",
        "\n",
        "    # Drop rows that are entirely empty\n",
        "    df = df.replace('', np.nan).dropna(how='all').replace(np.nan, '')\n",
        "    return df, detected_encoding\n",
        "\n",
        "\n",
        "# 3) Column Name Normalization & Mapping\n",
        "# ==============================================================================\n",
        "def normalize_column_names(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Normalizes DataFrame column names to lower_snake_case and de-duplicates them.\n",
        "    \"\"\"\n",
        "    new_columns = []\n",
        "    counts = {}\n",
        "    for col in df.columns:\n",
        "        # Handle non-string columns gracefully\n",
        "        if not isinstance(col, str):\n",
        "            col = str(col)\n",
        "        # Strip BOM/whitespace, normalize, and convert to snake_case\n",
        "        clean_col = re.sub(r'[\\s\\uFEFF]+', '_', zen_to_han(col.strip())).lower()\n",
        "        clean_col = re.sub(r'[^a-z0-9_]', '', clean_col)\n",
        "\n",
        "        # Deduplicate column names\n",
        "        if clean_col in counts:\n",
        "            counts[clean_col] += 1\n",
        "            new_columns.append(f\"{clean_col}_{counts[clean_col]}\")\n",
        "        else:\n",
        "            counts[clean_col] = 1\n",
        "            new_columns.append(clean_col)\n",
        "\n",
        "    df.columns = new_columns\n",
        "    return df\n",
        "\n",
        "\n",
        "def map_columns_to_schema(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Maps common Japanese and English column names to the target schema.\n",
        "    Unmapped columns are preserved with an 'auxiliary_' prefix.\n",
        "    \"\"\"\n",
        "    mapping = {\n",
        "        'date': ['日付', '取引日', 'ご利用日', '伝票日付', '予約日付'],\n",
        "        'description': ['内容', '摘要', '詳細', 'ご利用店名', 'メモ', '備考'],\n",
        "        'amount': ['金額', 'お支払金額', 'ご利用金額', '引落額', '入金額', '出金額'],\n",
        "        'balance': ['残高', '残高金額'],\n",
        "        'currency': ['通貨', '通貨コード'],\n",
        "        'category': ['分類', 'カテゴリ', 'カテゴリー']\n",
        "    }\n",
        "\n",
        "    # Invert mapping for faster lookup\n",
        "    inverted_mapping = {val: key for key, values in mapping.items() for val in values}\n",
        "\n",
        "    final_cols = {}\n",
        "    aux_cols = {}\n",
        "    for col in df.columns:\n",
        "        # Normalize original Japanese names for matching\n",
        "        normalized_col_name = col.replace('_', ' ').strip()\n",
        "\n",
        "        # Check against the direct Japanese mapping first\n",
        "        target_col = None\n",
        "        for k, v in mapping.items():\n",
        "            if any(jp_name in normalized_col_name for jp_name in v):\n",
        "                target_col = k\n",
        "                break\n",
        "\n",
        "        if target_col:\n",
        "            if target_col not in final_cols:\n",
        "                final_cols[target_col] = col\n",
        "            else:\n",
        "                # If target already mapped, move this one to auxiliary\n",
        "                aux_cols[f\"auxiliary_{col}\"] = col\n",
        "        else:\n",
        "            # If no match, it's an auxiliary column\n",
        "            aux_cols[f\"auxiliary_{col}\"] = col\n",
        "\n",
        "    # Rename columns based on the mapping\n",
        "    rename_dict = {v: k for k, v in final_cols.items()}\n",
        "    rename_dict.update({v: k for k, v in aux_cols.items()})\n",
        "    df = df.rename(columns=rename_dict)\n",
        "\n",
        "    print(\"INFO: Column mapping results:\")\n",
        "    for target, original in final_cols.items():\n",
        "        print(f\"  - Mapped '{original}' to '{target}'\")\n",
        "    for new_name, original in aux_cols.items():\n",
        "        print(f\"  - Kept '{original}' as '{new_name}'\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# 5) Date Parsing\n",
        "# ==============================================================================\n",
        "def parse_date_robustly(series: pd.Series) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Parses a Series of date strings into timezone-aware UTC datetimes.\n",
        "    \"\"\"\n",
        "    # First, handle Japanese era dates (if any) - simplified for this example\n",
        "    # A full implementation would use a library or more complex regex\n",
        "\n",
        "    # Use pandas' powerful to_datetime with multiple format attempts\n",
        "    parsed_dates = pd.to_datetime(series, errors='coerce', infer_datetime_format=True)\n",
        "\n",
        "    # Try a specific Japanese format for any remaining NaTs\n",
        "    jp_format = '%Y年%m月%d日'\n",
        "    still_nat = parsed_dates.isna()\n",
        "    if still_nat.any():\n",
        "        parsed_dates.loc[still_nat] = pd.to_datetime(\n",
        "            series[still_nat], format=jp_format, errors='coerce'\n",
        "        )\n",
        "\n",
        "    # Localize naive datetimes to Asia/Tokyo, then convert all to UTC\n",
        "    tokyo_tz = ZoneInfo(\"Asia/Tokyo\")\n",
        "    utc_tz = ZoneInfo(\"UTC\")\n",
        "\n",
        "    def to_utc(dt):\n",
        "        if pd.isna(dt):\n",
        "            return pd.NaT\n",
        "        if dt.tzinfo is None:\n",
        "            return dt.tz_localize(tokyo_tz).tz_convert(utc_tz)\n",
        "        return dt.tz_convert(utc_tz)\n",
        "\n",
        "    return parsed_dates.apply(to_utc)\n",
        "\n",
        "# 6) Currency & Amount Normalization\n",
        "# ==============================================================================\n",
        "def normalize_amount(series: pd.Series) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Cleans and converts a Series of amount strings to numeric floats.\n",
        "    Handles commas, currency symbols, full-width characters, and parentheses.\n",
        "    \"\"\"\n",
        "\n",
        "    def clean_single_amount(val):\n",
        "        if not isinstance(val, str) or val.strip() == '':\n",
        "            return np.nan\n",
        "\n",
        "        # Normalize characters\n",
        "        val = zen_to_han(val)\n",
        "\n",
        "        # Check for negative sign from parentheses\n",
        "        is_negative = '(' in val and ')' in val\n",
        "\n",
        "        # Remove all non-numeric characters except for decimal point and minus sign\n",
        "        val = re.sub(r'[^\\d.-]', '', val)\n",
        "\n",
        "        if not val:\n",
        "            return np.nan\n",
        "\n",
        "        try:\n",
        "            amount = float(val)\n",
        "            if is_negative and amount > 0:\n",
        "                amount *= -1\n",
        "            return amount\n",
        "        except ValueError:\n",
        "            return np.nan\n",
        "\n",
        "    return series.apply(clean_single_amount)\n",
        "\n",
        "\n",
        "def infer_amount_sign(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Infers amount sign (negative for expenses) based on keywords in description.\n",
        "    \"\"\"\n",
        "    df['amount'] = pd.to_numeric(df['amount'], errors='coerce')\n",
        "\n",
        "    negative_keywords = ['引落', '支払', '出金', 'fee', 'charge', 'payment']\n",
        "\n",
        "    def check_sign(row):\n",
        "        amount = row['amount']\n",
        "        desc = row.get('description', '').lower()\n",
        "        if pd.isna(amount):\n",
        "            return amount\n",
        "\n",
        "        if amount > 0:\n",
        "            if any(keyword in desc for keyword in negative_keywords):\n",
        "                return -amount\n",
        "        return amount\n",
        "\n",
        "    df['amount'] = df.apply(check_sign, axis=1)\n",
        "    return df\n",
        "\n",
        "# Main ETL Function\n",
        "# ==============================================================================\n",
        "def process_financial_csv(file_path_str: str) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Main ETL pipeline to process a financial transaction CSV.\n",
        "    \"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Starting ETL process for: {file_path_str}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    file_path = Path(file_path_str)\n",
        "    if not file_path.exists():\n",
        "        raise FileNotFoundError(f\"CRITICAL: The file {file_path_str} was not found.\")\n",
        "\n",
        "    # Step 2: Read file\n",
        "    df_raw, encoding = read_transaction_csv(file_path)\n",
        "    df = df_raw.copy()\n",
        "\n",
        "    # Step 3: Normalize columns and map to schema\n",
        "    df = normalize_column_names(df)\n",
        "    df = map_columns_to_schema(df)\n",
        "\n",
        "    # Step 4: Text Cleanup\n",
        "    if 'description' in df.columns:\n",
        "        df['original_description'] = df['description']\n",
        "        for col in df.select_dtypes(include=['object']).columns:\n",
        "            df[col] = df[col].astype(str).apply(zen_to_han).str.strip()\n",
        "            df[col] = df[col].str.replace(r'\\s+', ' ', regex=True)\n",
        "            df[col] = df[col].str.replace(r'[\\x00-\\x1F\\x7F]', '', regex=True)\n",
        "    else:\n",
        "        df['description'] = ''\n",
        "        df['original_description'] = ''\n",
        "        warnings.warn(\"WARNING: No 'description' column found or mapped.\")\n",
        "\n",
        "    # Step 5: Date Parsing\n",
        "    if 'date' in df.columns:\n",
        "        df['auxiliary_date_raw'] = df['date']\n",
        "        df['date'] = parse_date_robustly(df['date'])\n",
        "    else:\n",
        "        df['date'] = pd.NaT\n",
        "        warnings.warn(\"WARNING: No 'date' column found or mapped.\")\n",
        "\n",
        "    # Step 6: Amount Normalization\n",
        "    if 'amount' in df.columns:\n",
        "        df['amount'] = normalize_amount(df['amount'])\n",
        "        df = infer_amount_sign(df)\n",
        "    else:\n",
        "        df['amount'] = np.nan\n",
        "        warnings.warn(\"WARNING: No 'amount' column found or mapped.\")\n",
        "\n",
        "    # Step 6b: Currency\n",
        "    df['currency'] = 'JPY' # Defaulting as per spec\n",
        "\n",
        "    # Step 7: Balance Handling\n",
        "    if 'balance' in df.columns:\n",
        "        df['balance'] = normalize_amount(df['balance'])\n",
        "    else:\n",
        "        df['balance'] = np.nan\n",
        "\n",
        "    # Step 8: Category Handling\n",
        "    if 'category' not in df.columns:\n",
        "        df['category'] = None\n",
        "\n",
        "    # Step 9: Hash ID and Deduplication\n",
        "    print(\"\\n--- Deduplication ---\")\n",
        "    rows_before_dedupe = len(df)\n",
        "\n",
        "    # For hashing, ensure consistent data types and formats\n",
        "    df['hash_input'] = (\n",
        "        df['date'].dt.strftime('%Y-%m-%dT%H:%M:%SZ').fillna('') + '|' +\n",
        "        df['amount'].astype(str).fillna('') + '|' +\n",
        "        df['description'].astype(str).fillna('') + '|' +\n",
        "        df['currency'].astype(str).fillna('')\n",
        "    )\n",
        "    df['hash_id'] = df['hash_input'].apply(\n",
        "        lambda x: hashlib.sha256(x.encode('utf-8')).hexdigest()\n",
        "    )\n",
        "    df = df.drop(columns=['hash_input'])\n",
        "\n",
        "    df = df.drop_duplicates(subset=['hash_id'], keep='first')\n",
        "    rows_after_dedupe = len(df)\n",
        "    print(f\"Shape before deduplication: ({rows_before_dedupe}, {df.shape[1]})\")\n",
        "    print(f\"Shape after deduplication:  ({rows_after_dedupe}, {df.shape[1]})\")\n",
        "    print(f\"Removed {rows_before_dedupe - rows_after_dedupe} duplicate rows.\")\n",
        "\n",
        "    # Step 10: Final Assembly\n",
        "    df['source_file'] = str(file_path.resolve())\n",
        "    account_name = file_path.stem if file_path.stem else \"unknown\"\n",
        "    df['account'] = account_name\n",
        "\n",
        "    target_schema = [\n",
        "        \"date\", \"description\", \"original_description\", \"amount\", \"currency\",\n",
        "        \"balance\", \"category\", \"account\", \"source_file\", \"hash_id\"\n",
        "    ]\n",
        "\n",
        "    # Add any missing columns from the target schema\n",
        "    for col in target_schema:\n",
        "        if col not in df.columns:\n",
        "            df[col] = None\n",
        "\n",
        "    aux_cols = [col for col in df.columns if col.startswith('auxiliary_')]\n",
        "    final_ordered_cols = target_schema + sorted(aux_cols)\n",
        "    df = df[final_ordered_cols]\n",
        "\n",
        "    if df.empty:\n",
        "        warnings.warn(\"CRITICAL: DataFrame is empty after cleaning process. No output will be generated.\")\n",
        "        return None\n",
        "\n",
        "    return df\n",
        "\n",
        "# Main execution block\n",
        "if __name__ == \"__main__\":\n",
        "    # Simulate the presence of the input file\n",
        "    csv_content = \"\"\"\n",
        "    ,,\"これは不要な行です\",,\n",
        "    \"銀行取引明細\",,,,,\n",
        "    \"口座: 普通 1234567\",,,,,\n",
        "    ,,,,,\n",
        "    日付,内容,摘要,お支払金額,入金額,残高\n",
        "    \"2023年４月１日\",\"振込　カ）ＡＢＣ\",\"給与\",,\"350,000\", \"￥1,234,567\"\n",
        "    2023/05/10,\"ＡＴＭ引出\",\"現金\",\"-50000\",,\"1,184,567円\"\n",
        "    2023/05/15,\"カード利用　ＡＭＡＺＯＮ．ＣＯ．ＪＰ\",\"オンラインショッピング\",\"(12,800)\",,\n",
        "    2023-05-20,\"振込入金　ＸＹＺ社\",,,\"250,000\",1421767\n",
        "    ,\"デポジット\",,\"１０，０００\",,\n",
        "    2023/06/01,ＲＥＦＵＮＤ　ＦＲＯＭ　ＥＸＡＭＰＬＥ,返金,,1980,\"１，４３３，７４７\"\n",
        "    22/06/2023,ﾃﾞﾝｷﾘｮｳ ﾋｷｵﾄｼ,公共料金,\"(9,870)\",,１，４２３，８７７\n",
        "    2023年6月25日 15:30:00,振込　山田太郎,,(20000),,1403877\n",
        "    2023/06/25,\"振込　山田太郎\",,\"-20,000\",,1403877\n",
        "    null,無効なデータ,,,,,\n",
        "    2023/07/01,給与振込　カ）ＡＢＣ,給与,,\"300,000\",\"1,703,877\"\n",
        "    \"\"\"\n",
        "    input_file_path = Path(\"/mnt/data/取引.csv\")\n",
        "    input_file_path.parent.mkdir(exist_ok=True)\n",
        "    # Write with cp932 to test encoding detection\n",
        "    with open(input_file_path, \"w\", encoding=\"cp932\") as f:\n",
        "        f.write(csv_content.strip())\n",
        "\n",
        "    try:\n",
        "        df_clean = process_financial_csv(str(input_file_path))\n",
        "\n",
        "        if df_clean is not None:\n",
        "            # Step 11: Quality Checks & Clear Summaries\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"ETL Process Completed. Final DataFrame Summary:\")\n",
        "            print(\"=\"*60)\n",
        "\n",
        "            print(\"\\n--- Data Previews ---\")\n",
        "            print(\"df.head(10):\")\n",
        "            print(df_clean.head(10))\n",
        "            if len(df_clean) > 10:\n",
        "                print(\"\\ndf.sample(5, random_state=42):\")\n",
        "                print(df_clean.sample(min(5, len(df_clean)), random_state=42))\n",
        "\n",
        "            print(\"\\n--- Data Quality Report ---\")\n",
        "            null_report = pd.DataFrame({\n",
        "                'null_count': df_clean.isnull().sum(),\n",
        "                'null_percentage': (df_clean.isnull().sum() / len(df_clean) * 100)\n",
        "            })\n",
        "            print(null_report)\n",
        "\n",
        "            print(\"\\n--- Financial Summary ---\")\n",
        "            total_income = df_clean[df_clean['amount'] > 0]['amount'].sum()\n",
        "            total_expense = df_clean[df_clean['amount'] < 0]['amount'].sum()\n",
        "            min_date = df_clean['date'].min()\n",
        "            max_date = df_clean['date'].max()\n",
        "\n",
        "            print(f\"Date Range (UTC): {min_date.strftime('%Y-%m-%d')} to {max_date.strftime('%Y-%m-%d')}\")\n",
        "            print(f\"Total Income:  {total_income:,.2f} JPY\")\n",
        "            print(f\"Total Expense: {abs(total_expense):,.2f} JPY\")\n",
        "            print(f\"Net Flow:      {(total_income + total_expense):,.2f} JPY\")\n",
        "\n",
        "            print(\"\\n--- Monthly Rollup ---\")\n",
        "            monthly_rollup = df_clean.set_index('date').groupby(pd.Grouper(freq='M')).agg(\n",
        "                count=('amount', 'size'),\n",
        "                total_inflow=('amount', lambda x: x[x > 0].sum()),\n",
        "                total_outflow=('amount', lambda x: abs(x[x < 0].sum())),\n",
        "            )\n",
        "            monthly_rollup['net_flow'] = monthly_rollup['total_inflow'] - monthly_rollup['total_outflow']\n",
        "            print(monthly_rollup)\n",
        "\n",
        "            # Warnings\n",
        "            if (df_clean['amount'] > 0).all() or (df_clean['amount'] < 0).all():\n",
        "                warnings.warn(\"WARNING: All amounts are positive or all are negative. Sign inference may be incomplete.\")\n",
        "            if null_report.loc['date', 'null_percentage'] > 5:\n",
        "                 warnings.warn(f\"WARNING: More than 5% of dates failed to parse ({null_report.loc['date', 'null_percentage']:.2f}%).\")\n",
        "            if null_report.loc['amount', 'null_percentage'] > 5:\n",
        "                 warnings.warn(f\"WARNING: More than 5% of amounts failed to parse ({null_report.loc['amount', 'null_percentage']:.2f}%).\")\n",
        "\n",
        "\n",
        "            # Step 12: Save Artifacts\n",
        "            output_path = Path(\"/mnt/data/transactions_normalized.csv\")\n",
        "            df_clean.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(f\"Saved normalized CSV to {output_path} ({len(df_clean)} rows).\")\n",
        "            print(\"=\"*60)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nCRITICAL ERROR: The ETL process failed. Reason: {e}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Starting ETL process for: /mnt/data/取引.csv\n",
            "============================================================\n",
            "\n",
            "CRITICAL ERROR: The ETL process failed. Reason: CRITICAL: Could not read the file at /mnt/data/取引.csv with any of the attempted encodings.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-978063598.py:127: UserWarning: WARNING: Failed to read with encoding 'utf-8-sig': 'utf-8' codec can't decode byte 0x82 in position 3: invalid start byte\n",
            "  warnings.warn(f\"WARNING: Failed to read with encoding '{encoding}': {e}\")\n",
            "/tmp/ipython-input-978063598.py:127: UserWarning: WARNING: Failed to read with encoding 'utf-8': Error tokenizing data. C error: Expected 5 fields in line 2, saw 6\n",
            "\n",
            "  warnings.warn(f\"WARNING: Failed to read with encoding '{encoding}': {e}\")\n",
            "/tmp/ipython-input-978063598.py:127: UserWarning: WARNING: Failed to read with encoding 'cp932': Error tokenizing data. C error: Expected 5 fields in line 2, saw 6\n",
            "\n",
            "  warnings.warn(f\"WARNING: Failed to read with encoding '{encoding}': {e}\")\n",
            "/tmp/ipython-input-978063598.py:127: UserWarning: WARNING: Failed to read with encoding 'shift_jis': Error tokenizing data. C error: Expected 5 fields in line 2, saw 6\n",
            "\n",
            "  warnings.warn(f\"WARNING: Failed to read with encoding '{encoding}': {e}\")\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCqNRG4UPhnP",
        "outputId": "d084e6c8-dedb-4a7f-b3e8-5bf3c2779ccd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8JGeoUESRMf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ML1VKgevRaqU",
        "outputId": "9cc80092-dffc-40d7-fd23-248e9fede510"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TveKPpteYMSW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}